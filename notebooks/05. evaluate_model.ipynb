{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import (mean_absolute_error,\n",
    "                             mean_absolute_percentage_error,\n",
    "                             mean_squared_error, r2_score)\n",
    "                             \n",
    "from recommender_system.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH\n",
    "from recommender_system.utils import (create_directories, read_yaml,\n",
    "                                      unscale_targets)\n",
    "\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class EvaluateModelConfig:\n",
    "    \"\"\"Represents the configuration for evaluating the model.\"\"\"\n",
    "    root_dir: Path\n",
    "    trained_model_path: Path\n",
    "    data_path: Path\n",
    "    min_rating: float\n",
    "    max_rating: float\n",
    "    verbose: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, config_filepath=CONFIG_FILE_PATH, params_filepath=PARAMS_FILE_PATH\n",
    "    ):\n",
    "        \"\"\"Initialises ConfigurationManager with config and params filepaths.\"\"\"\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_evaluate_model_config(self) -> EvaluateModelConfig:\n",
    "        \"\"\"Returns the model evaluation configuration.\"\"\"\n",
    "        config = self.config.evaluate_model\n",
    "        trained_model_path = self.config.train_model.trained_model_path\n",
    "        data_path = self.config.data_preprocessing.root_dir\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        evaluate_model_config = EvaluateModelConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            trained_model_path=Path(trained_model_path),\n",
    "            data_path=Path(data_path),\n",
    "            min_rating=self.params.MIN_RATING,\n",
    "            max_rating=self.params.MAX_RATING,\n",
    "            verbose=self.params.VERBOSE\n",
    "        )\n",
    "\n",
    "        return evaluate_model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateModel:\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initialises the EvaluateModel object with the given config.\"\"\"\n",
    "        self.config = config\n",
    "\n",
    "    def load_trained_model(self):\n",
    "        \"\"\"Loads the trained model.\"\"\"\n",
    "        return tf.keras.models.load_model(self.config.trained_model_path)\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Loads and returns the required data for evaluation.\"\"\"\n",
    "        data_path = Path(self.config.data_path)\n",
    "        pickle_files = [\n",
    "            \"X_val.pkl\",\n",
    "            \"y_val_scaled.pkl\",\n",
    "            \"val_reviewer_ids.pkl\",\n",
    "            \"val_product_ids.pkl\"\n",
    "        ]\n",
    "\n",
    "        loaded_data = {}\n",
    "        for file_name in pickle_files:\n",
    "            file_path = data_path / file_name\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                loaded_data[file_name] = pickle.load(f)\n",
    "\n",
    "        X_val = loaded_data[\"X_val.pkl\"]\n",
    "        y_val_scaled = loaded_data[\"y_val_scaled.pkl\"]\n",
    "        self.y_val_original = unscale_targets(\n",
    "            y_val_scaled, self.config.min_rating, self.config.max_rating\n",
    "        )\n",
    "        self.val_reviewer_ids = loaded_data[\"val_reviewer_ids.pkl\"]\n",
    "        self.val_product_ids = loaded_data[\"val_product_ids.pkl\"]\n",
    "\n",
    "        return (\n",
    "            X_val,\n",
    "            y_val_scaled,\n",
    "            self.y_val_original,\n",
    "            self.val_reviewer_ids,\n",
    "            self.val_product_ids\n",
    "        )\n",
    "\n",
    "    def generate_predictions(self):\n",
    "        \"\"\"Generates predictions using the trained model.\"\"\"\n",
    "        model = self.load_trained_model()\n",
    "        X_val, _, _, _, _ = self.load_data()\n",
    "        self.y_hat = model.predict(X_val, verbose=self.config.verbose)\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Calculates evaluation metrics between ground truth y and predicted y_hat.\"\"\"\n",
    "        _, self.y, _, _, _ = self.load_data()\n",
    "        evaluation_results = {\n",
    "            \"R2\": r2_score(self.y, self.y_hat),\n",
    "            \"MAE\": mean_absolute_error(self.y, self.y_hat),\n",
    "            \"MAPE\": mean_absolute_percentage_error(self.y, self.y_hat),\n",
    "            \"MSE\": mean_squared_error(self.y, self.y_hat),\n",
    "            \"RMSE\": np.sqrt(mean_squared_error(self.y, self.y_hat))\n",
    "        }\n",
    "\n",
    "        results_filepath = Path(self.config.root_dir) / \"evaluation_results.json\"\n",
    "        with open(results_filepath, \"w\") as f:\n",
    "            json.dump(evaluation_results, f)\n",
    "\n",
    "        return evaluation_results\n",
    "\n",
    "    def calculate_dcg(self, relevance_scores, k):\n",
    "        \"\"\"Calculates Discounted Cumulative Gain (DCG) at position k.\"\"\"\n",
    "        relevance_scores = relevance_scores[:k]\n",
    "        positions = np.arange(2, len(relevance_scores) + 2)\n",
    "        dcg = np.sum(relevance_scores / np.log2(positions))\n",
    "        return dcg\n",
    "\n",
    "    def calculate_ndcg(self, relevance_scores, k):\n",
    "        \"\"\"Calculates Normalised Discounted Cumulative Gain (NDCG) at position k.\"\"\"\n",
    "        sorted_scores = sorted(relevance_scores, reverse=True)\n",
    "        dcg_max = self.calculate_dcg(sorted_scores, k)\n",
    "        dcg = self.calculate_dcg(relevance_scores, k)\n",
    "        ndcg = dcg / dcg_max if dcg_max != 0 else 0\n",
    "        return ndcg\n",
    "\n",
    "    def ranking_evaluation(self, k=10):\n",
    "        \"\"\"Performs ranking evaluation using NDCG.\"\"\"\n",
    "        ndcgs = []\n",
    "\n",
    "        for reviewer in np.unique(self.val_reviewer_ids):\n",
    "            target_val_product_ids = self.val_product_ids[\n",
    "                self.val_reviewer_ids == reviewer\n",
    "            ]\n",
    "            target_val_ratings = self.y_val_original[self.val_reviewer_ids == reviewer]\n",
    "            ndcg = self.calculate_ndcg(\n",
    "                target_val_ratings[\n",
    "                    np.argsort(-self.y_hat[self.val_reviewer_ids == reviewer])\n",
    "                ],\n",
    "                k=k\n",
    "            )\n",
    "            ndcgs.append(ndcg)\n",
    "\n",
    "        ndcg = np.mean(ndcgs)\n",
    "\n",
    "        results_filepath = Path(self.config.root_dir) / \"ndcg_result.json\"\n",
    "        with open(results_filepath, \"w\") as f:\n",
    "            json.dump({\"NDCG\": ndcg}, f)\n",
    "\n",
    "        return ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config_manager = ConfigurationManager()\n",
    "    evaluation_config = config_manager.get_evaluate_model_config()\n",
    "    evaluate_model = EvaluateModel(evaluation_config)\n",
    "    evaluate_model.generate_predictions()\n",
    "    evaluation_results = evaluate_model.evaluate()\n",
    "    ranking_results = evaluate_model.ranking_evaluation()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
